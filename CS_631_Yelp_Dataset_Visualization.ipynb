{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS_631_Yelp_Dataset_Visualization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va10j28zb05R"
      },
      "source": [
        "### Download Kaggle Dataset in Google Drive\n",
        "Reference：https://medium.com/analytics-vidhya/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a\n",
        "\n",
        "Kaggle access token is required to be put in your google drive, i.e. /MyDrive/Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ZQrzbiAGoD"
      },
      "source": [
        "Project Name: **Yelp Data Visualization and Analysis with\n",
        "Jupyter & Spark** \n",
        "\n",
        "Author: YING YAO, YING YU\n",
        "\n",
        "In order to properly run the code, please follow the weidgets instruction. It will prompt out the window to ask for permission and please authenticate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GILj1v2zT5VP"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QBFzD0XT_op"
      },
      "source": [
        "# Check current location, '/content' is the Colab virtual machine\n",
        "os.getcwd()\n",
        "# Enable the Kaggle environment, use the path to the directory your Kaggle API JSON is stored in\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/gdrive/MyDrive/Kaggle'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYOsELy5Uqiw"
      },
      "source": [
        "!pip install kaggle\n",
        "# Navigate into Drive where you want to store your Kaggle data\n",
        "if not os.path.exists('/gdrive/My Drive/Kaggle'):\n",
        "  os.mkdir('/gdrive/My Drive/Kaggle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LPgMNHu5nC5"
      },
      "source": [
        "!!! Next step is to download your Kaggle API token kaggle.json and save it in /gdrive/My Drive/Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr553GXa5mVe"
      },
      "source": [
        "os.chdir('/gdrive/My Drive/Kaggle')\n",
        "# Paste and run the copied API command, the data will download to the current directory\n",
        "!kaggle datasets download -d yelp-dataset/yelp-dataset --force\n",
        "# Check contents of directory, you should see the .zip file for the competition in your Drive\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u713nEAlb76v"
      },
      "source": [
        "### Mount Google Drive and Unzip Data in VM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmNuV_o5asFJ"
      },
      "source": [
        "# Complete path to storage location of the .zip file of data\n",
        "\n",
        "zip_path = '/gdrive/MyDrive/Kaggle/yelp-dataset.zip'\n",
        "vm_path = '/content'\n",
        "# Check current directory (be sure you're in the directory where Colab operates: '/content')\n",
        "os.chdir(vm_path)\n",
        "# Copy the .zip file into the present directory\n",
        "!cp '{zip_path}' .\n",
        "# Unzip quietly \n",
        "!unzip -q 'yelp-dataset.zip'\n",
        "# View the unzipped contents in the virtual machine\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS8PI0TboOD2"
      },
      "source": [
        "### Install Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhGIYhKxoV4t"
      },
      "source": [
        "%%time\n",
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz # effective on 2021/07/30\n",
        "!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# conf = SparkConf().setMaster(\"local[*]\").setAppName(\"MyTest\")\n",
        "# sc = SparkContext(conf=conf)\n",
        "# spark = SparkSession(sc)\n",
        "spark = SparkSession.builder \\\n",
        ".appName('app_name') \\\n",
        ".master('local[*]') \\\n",
        ".config('spark.sql.execution.arrow.pyspark.enabled', True) \\\n",
        ".config('spark.sql.session.timeZone', 'UTC') \\\n",
        ".config('spark.driver.memory','512G') \\\n",
        ".config('spark.ui.showConsoleProgress', True) \\\n",
        ".config('spark.sql.repl.eagerEval.enabled', True) \\\n",
        ".getOrCreate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjwWoTPHqa-w"
      },
      "source": [
        "### Initial Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx4zQhUdqgiE"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "from collections import Counter\n",
        "import pyspark.sql.functions as f\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.offline as py\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "# Using plotly + cufflinks in offline mode\n",
        "import cufflinks\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKuRAvVexr3E"
      },
      "source": [
        "%%time\n",
        "df_business = spark.read.json(\"/content/yelp_academic_dataset_business.json\").cache()\n",
        "#df_checkin = spark.read.json(\"/content/yelp_academic_dataset_checkin.json\").cache()\n",
        "df_review = spark.read.json(\"/content/yelp_academic_dataset_review.json\").cache()\n",
        "# df_tip = spark.read.json(\"/content/yelp_academic_dataset_tip.json\").cache()\n",
        "# df_user = spark.read.json(\"/content/yelp_academic_dataset_user.json\").cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N1eEDdrRSCy"
      },
      "source": [
        "# remove missing\n",
        "key_cols = ['name', 'address', 'categories', 'stars']\n",
        "df_business_1 = df_business.na.drop(subset = key_cols)\n",
        "print('% of missing values are {:.2f} %'.format((1 - df_business_1.count() / df_business.count()) * 100))\n",
        "# remove duplicates\n",
        "df_business_f = df_business_1.dropDuplicates(['name', 'latitude', 'longitude'])\n",
        "print('Removed {} duplicated business stores'.format(df_business_1.count() - df_business_f.count()))\n",
        "# TODO: add text processing for typo, e.g. resturants, restaurant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSVUIBC9fx7H"
      },
      "source": [
        "# get # of business per state\n",
        "bus_per_state = df_business_f.groupBy('state').count().orderBy('count', ascending=False).toPandas()\n",
        "\n",
        "# visualized the review count by state\n",
        "df = px.data.tips()\n",
        "fig = px.bar(x=bus_per_state['state'], y=bus_per_state['count'], labels={'x':'state', 'y':'count'}, \n",
        "             log_y=True, title=\"Business count per state\")\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggap6ePGuCCL"
      },
      "source": [
        "rdd_cat = df_business_f.select(f.split(df_business_f['categories'], ', ')).rdd.flatMap(lambda x: x).filter(lambda x: x is not None).flatMap(lambda x: x).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False)\n",
        "print('Total number of categories/subcategories are {}'.format(rdd_cat.count()))\n",
        "print('List categories/subcategories: {}'.format(rdd_cat.collect()))\n",
        "\n",
        "d = dict(rdd_cat.collect()[0:150])\n",
        "wordcloud = WordCloud (\n",
        "                    background_color = 'white',\n",
        "                    width = 1800,\n",
        "                    height = 1500\n",
        "                        ).generate_from_frequencies(d)\n",
        "plt.figure(figsize=(20,10),facecolor='k' )\n",
        " \n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off') # to off the axis of x and y\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Sua1RNhmAI"
      },
      "source": [
        "### Interactive Map Demo\n",
        "Reference: https://plotly.com/python/figurewidget-app/,\n",
        "\n",
        "https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "857y5H4edzeL"
      },
      "source": [
        "# create interactive plot\n",
        "dist_state = df_business_f.select('state').distinct().rdd.map(lambda x: x[0]).collect()\n",
        "top_cat = rdd_cat.map(lambda x: x[0]).collect()[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl4iSFgPqZzi"
      },
      "source": [
        "@interact_manual\n",
        "def interactive_store_map(state=dist_state, stars = np.arange(0, 5.1, 1), categories=top_cat):\n",
        "  df = df_business_f.filter((df_business_f['stars'] >= stars) & (df_business_f['state'] == state) & \n",
        "                            df_business_f['categories'].contains(categories))\n",
        "  fig = px.scatter_mapbox(df.toPandas(), lat=\"latitude\", lon=\"longitude\", hover_data=[\"name\", \"address\", \"postal_code\"], color=\"stars\", size=\"review_count\",\n",
        "                color_continuous_scale=px.colors.sequential.Rainbow, size_max=40, zoom=9, height=1000) # zoom level: https://wiki.openstreetmap.org/wiki/Zoom_levels\n",
        "  fig.update_layout(mapbox_style=\"open-street-map\")\n",
        "  fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "  # TODO: add city UI based on state\n",
        "  # TODO: add auto zoom-in on map based on long&lat\n",
        "  # get top 10 by starts and # of reviews\n",
        "  display('Top 10 recommendations are as below')\n",
        "  display(df.sort(df.stars.desc(), df.review_count.desc()).drop('attributes', 'business_id', 'latitude', 'longitude').show(10))\n",
        "  if df_business_f.count() == 0:\n",
        "    print('No store found based on the criteria')\n",
        "  py.iplot(fig, filename='Yelp Dataset Visualization')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt0nitIaDAdX"
      },
      "source": [
        "### Performance comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZFdxJuuEuBJ"
      },
      "source": [
        "#### Buinsess set (118 MB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWrtpnMTzMaA"
      },
      "source": [
        "Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijdZO0PwDEMB"
      },
      "source": [
        "start_time = time.time()\n",
        "df_business = spark.read.json(\"/content/yelp_academic_dataset_business.json\").cache()\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "# remove missing\n",
        "start_time = time.time()\n",
        "key_cols = ['name', 'address', 'categories', 'stars']\n",
        "df_business_1 = df_business.na.drop(subset = key_cols)\n",
        "print('% of missing values are {:.2f} %'.format((1 - df_business_1.count() / df_business.count()) * 100))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "# remove duplicates\n",
        "start_time = time.time()\n",
        "df_business_f = df_business_1.dropDuplicates(['name', 'latitude', 'longitude'])\n",
        "print('Removed {} duplicated business stores'.format(df_business_1.count() - df_business_f.count()))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "# calculate categories\n",
        "start_time = time.time()\n",
        "rdd_cat = df_business_f.select(f.split(df_business_f['categories'], ', ')).rdd.flatMap(lambda x: x).filter(lambda x: x is not None).flatMap(lambda x: x).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False)\n",
        "print('Total number of categories/subcategories are {}'.format(rdd_cat.count()))\n",
        "print('List categories/subcategories: {}'.format(rdd_cat.collect()))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "# create interactive plot\n",
        "dist_state = df_business_f.select('state').distinct().rdd.map(lambda x: x[0]).collect()\n",
        "top_cat = rdd_cat.map(lambda x: x[0]).collect()[0:100]\n",
        "@interact_manual\n",
        "def interactive_store_map(state=dist_state, stars = [0,1,2,3,4,5], categories=top_cat):\n",
        "  df = df_business_f.filter((df_business_f['stars'] >= stars) & (df_business_f['state'] == state) & \n",
        "                            df_business_f['categories'].contains(categories))\n",
        "  fig = px.scatter_mapbox(df.toPandas(), lat=\"latitude\", lon=\"longitude\", hover_data=[\"name\", \"address\", \"postal_code\"], color=\"stars\", size=\"review_count\",\n",
        "                color_continuous_scale=px.colors.sequential.Rainbow, size_max=40, zoom=9, height=1000) # zoom level: https://wiki.openstreetmap.org/wiki/Zoom_levels\n",
        "  fig.update_layout(mapbox_style=\"open-street-map\")\n",
        "  fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "  # TODO: add city UI based on state\n",
        "  # TODO: add auto zoom-in on map based on long&lat\n",
        "  # get top 10 by starts and # of reviews\n",
        "  display('Top 10 recommendations are as below')\n",
        "  display(df.sort(df.stars.desc(), df.review_count.desc()).drop('attributes', 'business_id', 'latitude', 'longitude').show(10))\n",
        "  if df.count() == 0:\n",
        "    print('No store found based on the criteria')\n",
        "  py.iplot(fig, filename='Yelp Dataset Visualization')\n",
        "\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn21A3-_Ggwv"
      },
      "source": [
        "Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjfVd3r-GcHi"
      },
      "source": [
        "start_time = time.time()\n",
        "df_business = pd.read_json(\"/content/yelp_academic_dataset_business.json\", lines=True)\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "# remove missing\n",
        "start_time = time.time()\n",
        "key_cols = ['name', 'address', 'categories', 'stars']\n",
        "df_business_1 = df_business.dropna(subset = key_cols)\n",
        "print('% of missing values are {:.2f} %'.format((1 - len(df_business_1)/ len(df_business)) * 100))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "# remove duplicates\n",
        "start_time = time.time()\n",
        "df_business_f = df_business_1.drop_duplicates(subset = ['name', 'latitude', 'longitude'])\n",
        "print('Removed {} duplicated business stores'.format(len(df_business_1) - len(df_business_f)))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "# calculate categories\n",
        "start_time = time.time()\n",
        "cat_list = []\n",
        "for i in df_business_f['categories'][df_business_f['categories'].notnull()].str.split(', '):\n",
        "  cat_list += i\n",
        "dict_cat = Counter(cat_list)\n",
        "sorted_dict_cat = sorted(dict_cat.items(), key=lambda x: x[1], reverse=True)\n",
        "print('Total number of categories/subcategories are {}'.format(len(sorted_dict_cat)))\n",
        "print('Dictionary of categories/subcategories: {}'.format(sorted_dict_cat))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "# create interactive plot\n",
        "start_time = time.time()\n",
        "dist_state = df_business_f['state'].unique()\n",
        "top_cat = [i[0] for i in sorted_dict_cat[:100]]\n",
        "@interact_manual\n",
        "def interactive_store_map(state=dist_state, stars = [0,1,2,3,4,5], categories=top_cat):\n",
        "  df = df_business_f[(df_business_f['stars'] >= stars) & (df_business_f['state'] == state) & \n",
        "                            (df_business_f['categories'].str.contains(categories))]\n",
        "  fig = px.scatter_mapbox(df, lat=\"latitude\", lon=\"longitude\", hover_data=[\"name\", \"address\", \"postal_code\"], color=\"stars\", size=\"review_count\",\n",
        "                color_continuous_scale=px.colors.sequential.Rainbow, size_max=40, zoom=9, height=1000) # zoom level: https://wiki.openstreetmap.org/wiki/Zoom_levels\n",
        "  fig.update_layout(mapbox_style=\"open-street-map\")\n",
        "  fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "  # TODO: add city UI based on state\n",
        "  # TODO: add auto zoom-in on map based on long&lat\n",
        "  # get top 10 by starts and # of reviews\n",
        "  display('Top 10 recommendations are as below')\n",
        "  display(df.sort_values(by = ['stars', 'review_count'], ascending=False).drop(columns = ['attributes', 'business_id', 'latitude', 'longitude'])[:10])\n",
        "  if len(df) == 0:\n",
        "    print('No store found based on the criteria')\n",
        "  py.iplot(fig, filename='Yelp Dataset Visualization')\n",
        "print('Time elapsed is {} second'.format(time.time() - start_time))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ZOTc32zSq9"
      },
      "source": [
        "#### Review set (6.5 G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnoSCwQQzYUc"
      },
      "source": [
        "Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMBzW2dzLkHI"
      },
      "source": [
        "start_time = time.time()\n",
        "df_review = spark.read.json(\"/content/yelp_academic_dataset_review.json\").cache()\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "df_review.show(5)\n",
        "# remove missing\n",
        "start_time = time.time()\n",
        "df_review_1 = df_review.na.drop(\"all\")\n",
        "print('% of missing values are {:.2f} %'.format((1 - df_review_1.count() / df_review.count()) * 100))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))\n",
        "\n",
        "# remove duplicates\n",
        "start_time = time.time()\n",
        "df_review_f = df_review_1.dropDuplicates()\n",
        "print('Removed {} duplicated review stores'.format(df_review_1.count() - df_review_f.count()))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMAOQt3azd6G"
      },
      "source": [
        "# # calculate categories\n",
        "# start_time = time.time()\n",
        "# rdd_cat = df_review_f.select(f.split(df_review_f['text'], ', ')).rdd.flatMap(lambda x: x).filter(lambda x: x is not None).flatMap(lambda x: x).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False)\n",
        "# print('Total number of words in review are {}'.format(rdd_cat.count()))\n",
        "# print('Top 1000 frequent words in review are {}'.format(rdd_cat.take(1000)))\n",
        "# print('Time elapsed is {:.2f} second'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSG9QRp9zchS"
      },
      "source": [
        "Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5tKY02iYAMy"
      },
      "source": [
        "start_time = time.time()\n",
        "df_review= pd.read_json(\"/content/yelp_academic_dataset_review.json\", lines=True)\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iiln_THSJgY"
      },
      "source": [
        "# ！！！This line of code crashes Colab due to out of RAM issue, need to spin up a larger cluster\n",
        "# remove missing\n",
        "start_time = time.time()\n",
        "df_business_1 = df_business.dropna()\n",
        "print('% of missing values are {:.2f} %'.format((1 - len(df_business_1)/ len(df_business)) * 100))\n",
        "print('Time elapsed is {:.2f} second'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgBCgfYFBEB0"
      },
      "source": [
        "Word Cloud for five star reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gj-lTjBBGbm"
      },
      "source": [
        "five_star_word_count = df_review.filter(df_review.stars >=5.0).select('text').rdd.filter(lambda x: x is not None).map(lambda x: len(x[0].split())).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False).collect()\n",
        "x_only, y_only = zip(*five_star_word_count)\n",
        "print(x_only)\n",
        "#visualized the review count by state\n",
        "import plotly.express as px\n",
        "fig = px.bar(x=x_only,y=y_only, labels={'x':'five star review length', 'y':'count'})\n",
        "fig.show()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "#remove unnecessary key\n",
        "five_star_analysis = df_review.filter(df_review.stars >=5.0).select('text') \\\n",
        "                      .rdd.filter(lambda x: x is not None).map(lambda x: x[0].split()) \\\n",
        "                      .flatMap(lambda x: x).map(lambda x:x.lower()).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y) \\\n",
        "                      .filter(lambda x:x[1]>10).filter(lambda x: x[0]!='i') \\\n",
        "                      .filter(lambda x: x[0]!='the').filter(lambda x: x[0]!='and') \\\n",
        "                      .filter(lambda x: x[0]!='he').filter(lambda x: x[0]!='she') \\\n",
        "                      .filter(lambda x: x[0]!='a').filter(lambda x: x[0]!='an') \\\n",
        "                      .filter(lambda x: x[0]!='to').filter(lambda x: x[0]!='was') \\\n",
        "                      .filter(lambda x: x[0]!='of').filter(lambda x: x[0]!='is') \\\n",
        "                      .filter(lambda x: x[0]!='in').filter(lambda x: x[0]!='for') \\\n",
        "                      .filter(lambda x: x[0]!='with').filter(lambda x: x[0]!='my') \\\n",
        "                      .filter(lambda x: x[0]!='on').filter(lambda x: x[0]!='it') \\\n",
        "                      .filter(lambda x: x[0]!='they').filter(lambda x: x[0]!='that') \\\n",
        "                        .filter(lambda x: x[0]!='we').filter(lambda x: x[0]!='at') \\\n",
        "                        .filter(lambda x: x[0]!='re').filter(lambda x: x[0]!='this') \\\n",
        "                        .filter(lambda x: x[0]!='you').filter(lambda x: x[0]!='have').collect()\n",
        "\n",
        "     \n",
        "\n",
        "sorted_by_second = sorted(five_star_analysis, key=lambda tup: -tup[1])[:150]\n",
        "\n",
        "x_only, y_only = zip(*sorted_by_second)\n",
        "print(x_only)\n",
        "#visualized the review count by state\n",
        "import plotly.express as px\n",
        "fig = px.bar(x=x_only,y=y_only, labels={'x':'word', 'y':'count'})\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n",
        "rdd_cat_df = dict(five_star_analysis)\n",
        "\n",
        "\n",
        "wordcloud = WordCloud (\n",
        "                    background_color = 'white',\n",
        "                    width = 1800,\n",
        "                    height = 1500\n",
        "                        ).generate_from_frequencies(rdd_cat_df)\n",
        "plt.figure(figsize=(20,10),facecolor='k' )\n",
        " \n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off') # to off the axis of x and y\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDoArCSeBVaG"
      },
      "source": [
        "Word Cloud for One star reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeOT7tUNBW3b"
      },
      "source": [
        "one_star_word_count = df_review.filter(df_review.stars == 1.0).select('text').rdd.filter(lambda x: x is not None).map(lambda x: len(x[0].split())).map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], ascending=False).collect()\n",
        "x_only, y_only = zip(*one_star_word_count)\n",
        "print(x_only)\n",
        "#visualized the review count by state\n",
        "import plotly.express as px\n",
        "fig = px.bar(x=x_only,y=y_only, labels={'x':'one star review length', 'y':'count'})\n",
        "fig.show()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "#remove unnecessary key\n",
        "one_star_analysis = df_review.filter(df_review.stars == 1.0).select('text') \\\n",
        "                      .rdd.filter(lambda x: x is not None).map(lambda x: x[0].split()) \\\n",
        "                      .flatMap(lambda x: x).map(lambda x:x.lower()).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y) \\\n",
        "                      .filter(lambda x:x[1]>10).filter(lambda x: x[0]!='i') \\\n",
        "                      .filter(lambda x: x[0]!='the').filter(lambda x: x[0]!='and') \\\n",
        "                      .filter(lambda x: x[0]!='he').filter(lambda x: x[0]!='she') \\\n",
        "                      .filter(lambda x: x[0]!='a').filter(lambda x: x[0]!='an') \\\n",
        "                      .filter(lambda x: x[0]!='to').filter(lambda x: x[0]!='was') \\\n",
        "                      .filter(lambda x: x[0]!='of').filter(lambda x: x[0]!='is') \\\n",
        "                      .filter(lambda x: x[0]!='in').filter(lambda x: x[0]!='for') \\\n",
        "                      .filter(lambda x: x[0]!='with').filter(lambda x: x[0]!='my') \\\n",
        "                      .filter(lambda x: x[0]!='on').filter(lambda x: x[0]!='it') \\\n",
        "                      .filter(lambda x: x[0]!='they').filter(lambda x: x[0]!='that') \\\n",
        "                      .filter(lambda x: x[0]!='we').filter(lambda x: x[0]!='at') \\\n",
        "                      .filter(lambda x: x[0]!='re').filter(lambda x: x[0]!='this') \\\n",
        "                      .filter(lambda x: x[0]!='you').filter(lambda x: x[0]!='have').collect()\n",
        "\n",
        "     \n",
        "\n",
        "sorted_by_second = sorted(one_star_analysis, key=lambda tup: -tup[1])[:150]\n",
        "\n",
        "x_only, y_only = zip(*sorted_by_second)\n",
        "print(x_only)\n",
        "#visualized the review count by state\n",
        "import plotly.express as px\n",
        "fig = px.bar(x=x_only,y=y_only, labels={'x':'word', 'y':'count'})\n",
        "fig.update_layout(yaxis_range=[0,1000000])\n",
        "\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n",
        "rdd_cat_df = dict(five_star_analysis)\n",
        "\n",
        "\n",
        "wordcloud = WordCloud (\n",
        "                    background_color = 'white',\n",
        "                    width = 1800,\n",
        "                    height = 1500\n",
        "                        ).generate_from_frequencies(rdd_cat_df)\n",
        "plt.figure(figsize=(20,10),facecolor='k' )\n",
        " \n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off') # to off the axis of x and y\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_g7-lCfBeR4"
      },
      "source": [
        "PMI Analysis for five star reviews\n",
        "\n",
        "Reference: https://student.cs.uwaterloo.ca/~cs451/assignments.html\n",
        "\n",
        "log(p(a,b) / ( p(a) * p(b) ))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84sYXNRBBf0y"
      },
      "source": [
        "#we reference the idea of homework of calculating the PMI\n",
        "#PMI Analysis for five star reviews one token\n",
        "\n",
        "top50 = df_review.filter(df_review.stars >=5.0).select('text') \\\n",
        "                      .rdd.filter(lambda x: x is not None).map(lambda x: x[0].split()).cache()\n",
        "words = top50.flatMap(lambda x: x).cache()\n",
        "total = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b).count()                  \n",
        "total\n",
        "\n",
        "word = top50.map(lambda x: list(set(x))).flatMap(lambda line:line) \\\n",
        "          .map(lambda word: (word, 1)).reduceByKey(lambda a,b:a+b).map(lambda x:(x[1]/total,x[1], x[0])).sortBy(lambda x: -x[1])\n",
        "word.take(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPBFfkY2Byuh"
      },
      "source": [
        "#PMI Analysis for five star reviews token pairs\n",
        "import math\n",
        "import collections\n",
        "import itertools\n",
        "# top50 = df_review.filter(df_review.stars >=5.0).select('text') \\\n",
        "#                       .rdd.filter(lambda x: x is not None).map(lambda x: x[0].split()).cache()\n",
        "# words = top50.flatMap(lambda x: x).cache()\n",
        "# total = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b).count()                  \n",
        "# total\n",
        "threshold = 6000\n",
        "word = top50.flatMap(lambda line:line).map(lambda word: (word, 1)) \\\n",
        "          .reduceByKey(lambda a,b:a+b).sortBy(lambda x: -x[1]) \\\n",
        "          .collect()\n",
        "def helper(input_v):\n",
        "    word1 = collections.Counter(word)\n",
        "    for i in word1:\n",
        "        if input_v in i:\n",
        "            return int(i[1])\n",
        "count_line_total = total\n",
        "token_pair = top50\n",
        "token_pair_all = token_pair.map(lambda x: list(set(x))) \\\n",
        "                  .map(lambda x: list(itertools.permutations(x,2))) \\\n",
        "                  .filter(lambda x: set(x)).flatMap(lambda x: x) \\\n",
        "                  .map(lambda word: (word, 1)).reduceByKey(lambda a,b:a+b).cache()\n",
        "token_pair_all = token_pair_all.filter(lambda x: x[1] >= threshold) \\\n",
        "                  .map(lambda x: (x[0], math.log10((count_line_total*x[1])/(helper(x[0][0])*helper(x[0][1]))),x[1],helper(x[0][0]),helper(x[0][1])))\n",
        "token_pair_all.take(50)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}